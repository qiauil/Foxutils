{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_dim: int=2, out_dim: int=2, h_dims: List[int]=[512]*4) -> None:\n",
    "        super().__init__()        \n",
    "        ins = [in_dim] + h_dims\n",
    "        outs = h_dims + [out_dim]\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Sequential(nn.Linear(in_d, out_d), nn.LeakyReLU()) for in_d, out_d in zip(ins, outs)]\n",
    "            )\n",
    "        self.out = nn.Sequential(nn.Linear(out_dim, out_dim))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "net=Net(1,1,[64]*4)\n",
    "x_train = torch.rand((1000,1))\n",
    "y_train = torch.sin(x_train*2*3.14159)\n",
    "x_val = torch.rand((100,1))\n",
    "y_val = torch.sin(x_val*2*3.14159)\n",
    "train_data = torch.cat((x_train, y_train), dim=1)\n",
    "val_data = torch.cat((x_val, y_val), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mandatory Configuration:\n",
      "    batch_size_train (int): Batch size for training.\n",
      "    lr (float): Initial learning rate.\n",
      "    num_epochs (int): Number of epochs for training.\n",
      "    project_path (str): Path to save the training results.\n",
      "\n",
      "Optional Configuration:\n",
      "    batch_size_val (int): Batch size for validation. Default is the same as batch_size_train. If not set, the batch size will be the same as the training batch size.\n",
      "    checkpoint_save_frequency (int): Frequency of saving checkpoints. The unit is epoch. If not set, the checkpoint will be saved every num_epochs//10 epoch.\n",
      "    device_type (str, possible options: ['cpu', 'cuda', 'mps', 'gpu', 'tpu', 'auto'], default value: auto): The hardware to run on. Will pass to `accelerator` in `Fabric`.\n",
      "    do_ema_validation (bool, default value: True): Whether to perform additional validation after EMA weight update\n",
      "    ema_coef (float, default value: 0.9): Exponential moving average coefficient for EMA weights recording\n",
      "    ema_weights_update_freq (int, default value: 1): Frequency of EMA weight update. The unit is the number of optimization steps\n",
      "    final_lr (float): Final learning rate for lr_scheduler. If not set, the final learning rate will be the same as the initial learning rate.\n",
      "    grad_accum_steps (int, default value: 1): Number of gradient accumulation steps. The unit is iteration.\n",
      "    log_iteration_loss (bool, default value: False): Whether to log the loss of each iteration.\n",
      "    logger (str, possible options: ['TensorBoard', 'CSV'], default value: TensorBoard): Logger for training. If you want to use other loggers e.g., wandb, you need to reimplement the `initialize_loggers` method in the trainer class.\n",
      "    logger_configs (dict, default value: {}): Additional configuration of loggers\n",
      "    lr_scheduler (str, possible options: ['cosine', 'linear', 'constant'], default value: constant): Learning rate scheduler for training. It can be set to 'cosine', 'linear', or 'constant'. If you want to use other learning rate schedulers, you reimplement the `configure_lr_scheduler` method in the trainer class.\n",
      "    multi_devices_strategy (str, default value: auto): Strategy for how to run across multiple devices. Possible choices include: \"dp\", \"ddp\", \"ddp_spawn\", \"deepspeed\", \"fsdp\". Full list of available options can be found at https://lightning.ai/docs/fabric/stable/api/fabric_args.html#strategy. Will pass to `strategy` in `Fabric`.\n",
      "    num_id_devices (Union, default value: auto): Number of devices to train on (int), which GPUs to train on (list or str), or \"auto\".The value applies per node. Will pass to `device` in `Fabric`.\n",
      "    num_nodes (int, default value: 1): Number of GPU nodes for distributed training.\n",
      "    optimizer (str, default value: AdamW): Optimizer for training. Can be any optimizer in `torch.optim.`\n",
      "    optimizer_configs (dict, default value: {}): Additional configuration of optimizers\n",
      "    precision (str, default value: 32): Precision to use. Double precision (\"64\"), full precision (\"32\"), half precision AMP (\"16-mixed\"), or bfloat16 precision AMP (\"bf16-mixed\"). Full list of available options can be found at https://lightning.ai/docs/fabric/stable/api/fabric_args.html#precision. Will pass to `precision` in `Fabric`.\n",
      "    random_seed (int): Random seed for training. If not set, the random seed will be the current time.\n",
      "    run_in_silence (bool, default value: False): Whether to run in silence mode. If set to True, only warning will be printed to the console.\n",
      "    run_name (str): Name of the training run; If not set, the run_name will be set as current time.\n",
      "    save_model_structure (bool, default value: True): Whether to save the model structure.\n",
      "    show_iteration_bar (bool, default value: True): Whether to show training/validation iteration bar.\n",
      "    train_dataloader_configs (dict, default value: {}): Additional configuration of training dataloader\n",
      "    validation_dataloader_configs (dict, default value: {}): Additional configuration of validation dataloader\n",
      "    validation_frequency (int): Frequency of validation. The unit is epoch. If not set, will validate every num_epochs//10 epoch.\n",
      "    version (str): Version of the training run; If not set, the version will be set to \"sd_random_seed\"\n",
      "    warmup_epoch (int, default value: 0): Number of epochs for learning rate warm up.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mandatory Configuration:\\n    batch_size_train (int): Batch size for training.\\n    lr (float): Initial learning rate.\\n    num_epochs (int): Number of epochs for training.\\n    project_path (str): Path to save the training results.\\n\\nOptional Configuration:\\n    batch_size_val (int): Batch size for validation. Default is the same as batch_size_train. If not set, the batch size will be the same as the training batch size.\\n    checkpoint_save_frequency (int): Frequency of saving checkpoints. The unit is epoch. If not set, the checkpoint will be saved every num_epochs//10 epoch.\\n    device_type (str, possible options: [\\'cpu\\', \\'cuda\\', \\'mps\\', \\'gpu\\', \\'tpu\\', \\'auto\\'], default value: auto): The hardware to run on. Will pass to `accelerator` in `Fabric`.\\n    do_ema_validation (bool, default value: True): Whether to perform additional validation after EMA weight update\\n    ema_coef (float, default value: 0.9): Exponential moving average coefficient for EMA weights recording\\n    ema_weights_update_freq (int, default value: 1): Frequency of EMA weight update. The unit is the number of optimization steps\\n    final_lr (float): Final learning rate for lr_scheduler. If not set, the final learning rate will be the same as the initial learning rate.\\n    grad_accum_steps (int, default value: 1): Number of gradient accumulation steps. The unit is iteration.\\n    log_iteration_loss (bool, default value: False): Whether to log the loss of each iteration.\\n    logger (str, possible options: [\\'TensorBoard\\', \\'CSV\\'], default value: TensorBoard): Logger for training. If you want to use other loggers e.g., wandb, you need to reimplement the `initialize_loggers` method in the trainer class.\\n    logger_configs (dict, default value: {}): Additional configuration of loggers\\n    lr_scheduler (str, possible options: [\\'cosine\\', \\'linear\\', \\'constant\\'], default value: constant): Learning rate scheduler for training. It can be set to \\'cosine\\', \\'linear\\', or \\'constant\\'. If you want to use other learning rate schedulers, you reimplement the `configure_lr_scheduler` method in the trainer class.\\n    multi_devices_strategy (str, default value: auto): Strategy for how to run across multiple devices. Possible choices include: \"dp\", \"ddp\", \"ddp_spawn\", \"deepspeed\", \"fsdp\". Full list of available options can be found at https://lightning.ai/docs/fabric/stable/api/fabric_args.html#strategy. Will pass to `strategy` in `Fabric`.\\n    num_id_devices (Union, default value: auto): Number of devices to train on (int), which GPUs to train on (list or str), or \"auto\".The value applies per node. Will pass to `device` in `Fabric`.\\n    num_nodes (int, default value: 1): Number of GPU nodes for distributed training.\\n    optimizer (str, default value: AdamW): Optimizer for training. Can be any optimizer in `torch.optim.`\\n    optimizer_configs (dict, default value: {}): Additional configuration of optimizers\\n    precision (str, default value: 32): Precision to use. Double precision (\"64\"), full precision (\"32\"), half precision AMP (\"16-mixed\"), or bfloat16 precision AMP (\"bf16-mixed\"). Full list of available options can be found at https://lightning.ai/docs/fabric/stable/api/fabric_args.html#precision. Will pass to `precision` in `Fabric`.\\n    random_seed (int): Random seed for training. If not set, the random seed will be the current time.\\n    run_in_silence (bool, default value: False): Whether to run in silence mode. If set to True, only warning will be printed to the console.\\n    run_name (str): Name of the training run; If not set, the run_name will be set as current time.\\n    save_model_structure (bool, default value: True): Whether to save the model structure.\\n    show_iteration_bar (bool, default value: True): Whether to show training/validation iteration bar.\\n    train_dataloader_configs (dict, default value: {}): Additional configuration of training dataloader\\n    validation_dataloader_configs (dict, default value: {}): Additional configuration of validation dataloader\\n    validation_frequency (int): Frequency of validation. The unit is epoch. If not set, will validate every num_epochs//10 epoch.\\n    version (str): Version of the training run; If not set, the version will be set to \"sd_random_seed\"\\n    warmup_epoch (int, default value: 0): Number of epochs for learning rate warm up.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any\n",
    "from torch.nn.modules import Module\n",
    "from foxutils.trainer import EMATrainer as Trainer\n",
    "import time\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    \n",
    "    def train_step(self, model: Module, batch: Any, batch_idx: int):\n",
    "        inputs=batch[:,:1]\n",
    "        targets=batch[:,1:]\n",
    "        time.sleep(0.01)\n",
    "        return torch.nn.functional.mse_loss(model(inputs), targets)\n",
    "    \n",
    "    def validation_step(self, model: Module, batch: Any, batch_idx: int):\n",
    "        inputs=batch[:,:1]\n",
    "        targets=batch[:,1:]\n",
    "        time.sleep(0.1)\n",
    "        return torch.nn.functional.mse_loss(model(inputs), targets)\n",
    "    \n",
    "trainer=MyTrainer()\n",
    "trainer.info_available_configs(print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1103fe4d338d4e768e14a9c515452a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finished Epoch:  40%|####      | 40/100 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 14:38:13.478758: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-11 14:38:13.490396: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-11 14:38:13.504468: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-11 14:38:13.508452: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-11 14:38:13.518867: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-11 14:38:14.328497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "trainer.train(net, train_data, val_data,\n",
    "              project_path='./learn_sin',\n",
    "              num_epochs=100,\n",
    "              lr=1e-3,\n",
    "              batch_size_train=32,\n",
    "              run_name='test',\n",
    "              show_iteration_bar=False,\n",
    "              checkpoint_save_frequency=20,\n",
    "              do_ema_validation=False,\n",
    "              log_iteration_loss=False,\n",
    "              lr_scheduler=\"constant\",\n",
    "              optimizer=\"SGD\",\n",
    "              precision=\"64\",\n",
    "              random_seed=42,\n",
    "              save_model_structure=False,\n",
    "              ema_weights_update_freq=5,\n",
    "              version=\"my\",\n",
    "              warmup_epoch=50,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_mixin = TrainConfigMixin()\n",
    "config_mixin.register_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mandatory Configuration:\n",
      "    batch_size_train (int, group: Control): Batch size for training.\n",
      "    lr (float, group: Control): Initial learning rate.\n",
      "    num_epochs (int, group: Control): Number of epochs for training.\n",
      "    project_path (str, group: ProjectInfo): Path to save the training results.\n",
      "\n",
      "Optional Configuration:\n",
      "    batch_size_val (int, group: Control): Batch size for validation. Default is the same as batch_size_train. If not set, the batch size will be the same as the training batch size.\n",
      "    checkpoint_save_frequency (int, group: Control): Frequency of saving checkpoints. The unit is epoch. If not set, the checkpoint will be saved every num_epochs//10 epoch.\n",
      "    device_type (str, possible options: ['cpu', 'cuda', 'mps', 'gpu', 'tpu', 'auto'], default value: auto, group: Control): The hardware to run on. Will pass to `accelerator` in `Fabric`.\n",
      "    final_lr (float, group: Control): Final learning rate for lr_scheduler. If not set, the final learning rate will be the same as the initial learning rate.\n",
      "    grad_accum_steps (int, default value: 1, group: Control): Number of gradient accumulation steps. The unit is iteration.\n",
      "    log_iteration_loss (bool, default value: False, group: Logging): Whether to log the loss of each iteration.\n",
      "    logger (str, possible options: ['TensorBoard', 'CSV'], default value: TensorBoard, group: Logging): Logger for training. If you want to use other loggers e.g., wandb, you need to reimplement the `initialize_loggers` method in the trainer class.\n",
      "    logger_configs (dict, default value: {}, group: Logging): Additional configuration of loggers\n",
      "    lr_scheduler (str, possible options: ['cosine', 'linear', 'constant'], default value: constant, group: Control): Learning rate scheduler for training. It can be set to 'cosine', 'linear', or 'constant'. If you want to use other learning rate schedulers, you reimplement the `configure_lr_scheduler` method in the trainer class.\n",
      "    multi_devices_strategy (str, default value: auto, group: DistributedTraining): Strategy for how to run across multiple devices. Possible choices include: \"dp\", \"ddp\", \"ddp_spawn\", \"deepspeed\", \"fsdp\". Full list of available options can be found at https://lightning.ai/docs/fabric/stable/api/fabric_args.html#strategy. Will pass to `strategy` in `Fabric`.\n",
      "    num_id_devices (Union, default value: auto, group: DistributedTraining): Number of devices to train on (int), which GPUs to train on (list or str), or \"auto\".The value applies per node. Will pass to `device` in `Fabric`.\n",
      "    num_nodes (int, default value: 1, group: DistributedTraining): Number of GPU nodes for distributed training.\n",
      "    optimizer (str, default value: AdamW, group: Control): Optimizer for training. Can be any optimizer in `torch.optim.`\n",
      "    optimizer_configs (dict, default value: {}, group: Control): Additional configuration of optimizers\n",
      "    precision (str, default value: 32, group: Control): Precision to use. Double precision (\"64\"), full precision (\"32\"), half precision AMP (\"16-mixed\"), or bfloat16 precision AMP (\"bf16-mixed\"). Full list of available options can be found at https://lightning.ai/docs/fabric/stable/api/fabric_args.html#precision. Will pass to `precision` in `Fabric`.\n",
      "    random_seed (int, group: Control): Random seed for training. If not set, the random seed will be the current time.\n",
      "    run_in_silence (bool, default value: False, group: Logging): Whether to run in silence mode. If set to True, only warning will be printed to the console.\n",
      "    run_name (str, group: ProjectInfo): Name of the training run; If not set, the run_name will be set as current time.\n",
      "    save_model_structure (bool, default value: True, group: Logging): Whether to save the model structure.\n",
      "    show_iteration_bar (bool, default value: True, group: Logging): Whether to show training/validation iteration bar.\n",
      "    train_dataloader_configs (dict, default value: {}, group: Control): Additional configuration of training dataloader\n",
      "    validation_dataloader_configs (dict, default value: {}, group: Control): Additional configuration of validation dataloader\n",
      "    validation_frequency (int, group: Control): Frequency of validation. The unit is epoch. If not set, will validate every num_epochs//10 epoch.\n",
      "    version (str, group: ProjectInfo): Version of the training run; If not set, the version will be set to \"sd_random_seed\"\n",
      "    warmup_epoch (int, default value: 0, group: Control): Number of epochs for learning rate warm up.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mandatory Configuration:\\n    batch_size_train (int, group: Control): Batch size for training.\\n    lr (float, group: Control): Initial learning rate.\\n    num_epochs (int, group: Control): Number of epochs for training.\\n    project_path (str, group: ProjectInfo): Path to save the training results.\\n\\nOptional Configuration:\\n    batch_size_val (int, group: Control): Batch size for validation. Default is the same as batch_size_train. If not set, the batch size will be the same as the training batch size.\\n    checkpoint_save_frequency (int, group: Control): Frequency of saving checkpoints. The unit is epoch. If not set, the checkpoint will be saved every num_epochs//10 epoch.\\n    device_type (str, possible options: [\\'cpu\\', \\'cuda\\', \\'mps\\', \\'gpu\\', \\'tpu\\', \\'auto\\'], default value: auto, group: Control): The hardware to run on. Will pass to `accelerator` in `Fabric`.\\n    final_lr (float, group: Control): Final learning rate for lr_scheduler. If not set, the final learning rate will be the same as the initial learning rate.\\n    grad_accum_steps (int, default value: 1, group: Control): Number of gradient accumulation steps. The unit is iteration.\\n    log_iteration_loss (bool, default value: False, group: Logging): Whether to log the loss of each iteration.\\n    logger (str, possible options: [\\'TensorBoard\\', \\'CSV\\'], default value: TensorBoard, group: Logging): Logger for training. If you want to use other loggers e.g., wandb, you need to reimplement the `initialize_loggers` method in the trainer class.\\n    logger_configs (dict, default value: {}, group: Logging): Additional configuration of loggers\\n    lr_scheduler (str, possible options: [\\'cosine\\', \\'linear\\', \\'constant\\'], default value: constant, group: Control): Learning rate scheduler for training. It can be set to \\'cosine\\', \\'linear\\', or \\'constant\\'. If you want to use other learning rate schedulers, you reimplement the `configure_lr_scheduler` method in the trainer class.\\n    multi_devices_strategy (str, default value: auto, group: DistributedTraining): Strategy for how to run across multiple devices. Possible choices include: \"dp\", \"ddp\", \"ddp_spawn\", \"deepspeed\", \"fsdp\". Full list of available options can be found at https://lightning.ai/docs/fabric/stable/api/fabric_args.html#strategy. Will pass to `strategy` in `Fabric`.\\n    num_id_devices (Union, default value: auto, group: DistributedTraining): Number of devices to train on (int), which GPUs to train on (list or str), or \"auto\".The value applies per node. Will pass to `device` in `Fabric`.\\n    num_nodes (int, default value: 1, group: DistributedTraining): Number of GPU nodes for distributed training.\\n    optimizer (str, default value: AdamW, group: Control): Optimizer for training. Can be any optimizer in `torch.optim.`\\n    optimizer_configs (dict, default value: {}, group: Control): Additional configuration of optimizers\\n    precision (str, default value: 32, group: Control): Precision to use. Double precision (\"64\"), full precision (\"32\"), half precision AMP (\"16-mixed\"), or bfloat16 precision AMP (\"bf16-mixed\"). Full list of available options can be found at https://lightning.ai/docs/fabric/stable/api/fabric_args.html#precision. Will pass to `precision` in `Fabric`.\\n    random_seed (int, group: Control): Random seed for training. If not set, the random seed will be the current time.\\n    run_in_silence (bool, default value: False, group: Logging): Whether to run in silence mode. If set to True, only warning will be printed to the console.\\n    run_name (str, group: ProjectInfo): Name of the training run; If not set, the run_name will be set as current time.\\n    save_model_structure (bool, default value: True, group: Logging): Whether to save the model structure.\\n    show_iteration_bar (bool, default value: True, group: Logging): Whether to show training/validation iteration bar.\\n    train_dataloader_configs (dict, default value: {}, group: Control): Additional configuration of training dataloader\\n    validation_dataloader_configs (dict, default value: {}, group: Control): Additional configuration of validation dataloader\\n    validation_frequency (int, group: Control): Frequency of validation. The unit is epoch. If not set, will validate every num_epochs//10 epoch.\\n    version (str, group: ProjectInfo): Version of the training run; If not set, the version will be set to \"sd_random_seed\"\\n    warmup_epoch (int, default value: 0, group: Control): Number of epochs for learning rate warm up.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_mixin.info_available_configs(print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_mixin.set_config_items(project_path='./learn_sin',\n",
    "                              batch_size_train=32,\n",
    "                              lr=1e-3,\n",
    "                              num_epochs=1000,\n",
    "                              final_lr=1e-6,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_mixin.save_configs_to_yaml('./control')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'a': 1}\n"
     ]
    }
   ],
   "source": [
    "class A:\n",
    "    \n",
    "    _my_param2=None\n",
    "    \n",
    "    def change_param(self):\n",
    "            self._my_param2={\"a\":1}\n",
    "a=A()\n",
    "a2=A()\n",
    "a2.change_param()\n",
    "print(a._my_param2)\n",
    "print(a2._my_param2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'a':1}\n",
    "b = a.copy()\n",
    "\n",
    "b['a']=2\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
